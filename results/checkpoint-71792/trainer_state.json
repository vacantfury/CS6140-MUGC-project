{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 8.0,
  "eval_steps": 500,
  "global_step": 71792,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.05571651437486071,
      "grad_norm": 8.746732711791992,
      "learning_rate": 4.965177178515713e-05,
      "loss": 0.7174,
      "step": 500
    },
    {
      "epoch": 0.11143302874972141,
      "grad_norm": 4.2278242111206055,
      "learning_rate": 4.930354357031424e-05,
      "loss": 0.6865,
      "step": 1000
    },
    {
      "epoch": 0.16714954312458213,
      "grad_norm": 20.021316528320312,
      "learning_rate": 4.8955315355471366e-05,
      "loss": 0.6883,
      "step": 1500
    },
    {
      "epoch": 0.22286605749944283,
      "grad_norm": 14.482854843139648,
      "learning_rate": 4.8607087140628484e-05,
      "loss": 0.697,
      "step": 2000
    },
    {
      "epoch": 0.2785825718743035,
      "grad_norm": 4.104323387145996,
      "learning_rate": 4.82588589257856e-05,
      "loss": 0.6823,
      "step": 2500
    },
    {
      "epoch": 0.33429908624916427,
      "grad_norm": 2.266880750656128,
      "learning_rate": 4.791063071094273e-05,
      "loss": 0.6893,
      "step": 3000
    },
    {
      "epoch": 0.39001560062402496,
      "grad_norm": 14.261711120605469,
      "learning_rate": 4.756240249609985e-05,
      "loss": 0.6852,
      "step": 3500
    },
    {
      "epoch": 0.44573211499888565,
      "grad_norm": 12.075372695922852,
      "learning_rate": 4.7214174281256966e-05,
      "loss": 0.6813,
      "step": 4000
    },
    {
      "epoch": 0.5014486293737463,
      "grad_norm": 11.088624954223633,
      "learning_rate": 4.6865946066414085e-05,
      "loss": 0.6867,
      "step": 4500
    },
    {
      "epoch": 0.557165143748607,
      "grad_norm": 4.437972068786621,
      "learning_rate": 4.651771785157121e-05,
      "loss": 0.6884,
      "step": 5000
    },
    {
      "epoch": 0.6128816581234678,
      "grad_norm": 2.939385175704956,
      "learning_rate": 4.616948963672833e-05,
      "loss": 0.6709,
      "step": 5500
    },
    {
      "epoch": 0.6685981724983285,
      "grad_norm": 3.6554665565490723,
      "learning_rate": 4.582126142188545e-05,
      "loss": 0.6878,
      "step": 6000
    },
    {
      "epoch": 0.7243146868731892,
      "grad_norm": 3.570380926132202,
      "learning_rate": 4.547303320704257e-05,
      "loss": 0.6895,
      "step": 6500
    },
    {
      "epoch": 0.7800312012480499,
      "grad_norm": 13.450767517089844,
      "learning_rate": 4.5124804992199686e-05,
      "loss": 0.6859,
      "step": 7000
    },
    {
      "epoch": 0.8357477156229106,
      "grad_norm": 5.702710151672363,
      "learning_rate": 4.477657677735681e-05,
      "loss": 0.6893,
      "step": 7500
    },
    {
      "epoch": 0.8914642299977713,
      "grad_norm": 4.401089191436768,
      "learning_rate": 4.442834856251393e-05,
      "loss": 0.6828,
      "step": 8000
    },
    {
      "epoch": 0.947180744372632,
      "grad_norm": 7.271941184997559,
      "learning_rate": 4.408012034767105e-05,
      "loss": 0.6824,
      "step": 8500
    },
    {
      "epoch": 1.0,
      "eval_loss": 0.6911399364471436,
      "eval_runtime": 84.1608,
      "eval_samples_per_second": 106.629,
      "eval_steps_per_second": 13.332,
      "step": 8974
    },
    {
      "epoch": 1.0028972587474927,
      "grad_norm": 14.675689697265625,
      "learning_rate": 4.3731892132828175e-05,
      "loss": 0.6775,
      "step": 9000
    },
    {
      "epoch": 1.0586137731223535,
      "grad_norm": 4.439974784851074,
      "learning_rate": 4.3383663917985294e-05,
      "loss": 0.685,
      "step": 9500
    },
    {
      "epoch": 1.114330287497214,
      "grad_norm": 2.9584622383117676,
      "learning_rate": 4.303543570314241e-05,
      "loss": 0.6852,
      "step": 10000
    },
    {
      "epoch": 1.1700468018720749,
      "grad_norm": 4.890965938568115,
      "learning_rate": 4.268720748829954e-05,
      "loss": 0.6784,
      "step": 10500
    },
    {
      "epoch": 1.2257633162469357,
      "grad_norm": 11.53409194946289,
      "learning_rate": 4.233897927345665e-05,
      "loss": 0.6749,
      "step": 11000
    },
    {
      "epoch": 1.2814798306217963,
      "grad_norm": 6.228061199188232,
      "learning_rate": 4.1990751058613776e-05,
      "loss": 0.6759,
      "step": 11500
    },
    {
      "epoch": 1.337196344996657,
      "grad_norm": 4.715677738189697,
      "learning_rate": 4.16425228437709e-05,
      "loss": 0.675,
      "step": 12000
    },
    {
      "epoch": 1.3929128593715177,
      "grad_norm": 5.417860507965088,
      "learning_rate": 4.1294294628928013e-05,
      "loss": 0.6929,
      "step": 12500
    },
    {
      "epoch": 1.4486293737463785,
      "grad_norm": 11.575021743774414,
      "learning_rate": 4.094606641408514e-05,
      "loss": 0.6718,
      "step": 13000
    },
    {
      "epoch": 1.504345888121239,
      "grad_norm": 3.89030385017395,
      "learning_rate": 4.059783819924226e-05,
      "loss": 0.6815,
      "step": 13500
    },
    {
      "epoch": 1.5600624024960998,
      "grad_norm": 15.520987510681152,
      "learning_rate": 4.024960998439938e-05,
      "loss": 0.6842,
      "step": 14000
    },
    {
      "epoch": 1.6157789168709606,
      "grad_norm": 3.062546730041504,
      "learning_rate": 3.99013817695565e-05,
      "loss": 0.6835,
      "step": 14500
    },
    {
      "epoch": 1.6714954312458212,
      "grad_norm": 3.2282042503356934,
      "learning_rate": 3.955315355471362e-05,
      "loss": 0.6767,
      "step": 15000
    },
    {
      "epoch": 1.7272119456206818,
      "grad_norm": 11.634241104125977,
      "learning_rate": 3.920492533987074e-05,
      "loss": 0.6853,
      "step": 15500
    },
    {
      "epoch": 1.7829284599955426,
      "grad_norm": 2.4767322540283203,
      "learning_rate": 3.885669712502786e-05,
      "loss": 0.6734,
      "step": 16000
    },
    {
      "epoch": 1.8386449743704034,
      "grad_norm": 5.231618404388428,
      "learning_rate": 3.850846891018498e-05,
      "loss": 0.6867,
      "step": 16500
    },
    {
      "epoch": 1.8943614887452642,
      "grad_norm": 4.616730690002441,
      "learning_rate": 3.81602406953421e-05,
      "loss": 0.6733,
      "step": 17000
    },
    {
      "epoch": 1.9500780031201248,
      "grad_norm": 4.948040962219238,
      "learning_rate": 3.781201248049922e-05,
      "loss": 0.6849,
      "step": 17500
    },
    {
      "epoch": 2.0,
      "eval_loss": 0.6684218049049377,
      "eval_runtime": 84.5122,
      "eval_samples_per_second": 106.186,
      "eval_steps_per_second": 13.276,
      "step": 17948
    },
    {
      "epoch": 2.0057945174949854,
      "grad_norm": 5.419151306152344,
      "learning_rate": 3.746378426565634e-05,
      "loss": 0.6793,
      "step": 18000
    },
    {
      "epoch": 2.061511031869846,
      "grad_norm": 4.573594093322754,
      "learning_rate": 3.711555605081346e-05,
      "loss": 0.686,
      "step": 18500
    },
    {
      "epoch": 2.117227546244707,
      "grad_norm": 11.713921546936035,
      "learning_rate": 3.6767327835970585e-05,
      "loss": 0.6787,
      "step": 19000
    },
    {
      "epoch": 2.172944060619568,
      "grad_norm": 12.795859336853027,
      "learning_rate": 3.6419099621127704e-05,
      "loss": 0.6814,
      "step": 19500
    },
    {
      "epoch": 2.228660574994428,
      "grad_norm": 5.6124491691589355,
      "learning_rate": 3.607087140628482e-05,
      "loss": 0.6781,
      "step": 20000
    },
    {
      "epoch": 2.284377089369289,
      "grad_norm": 4.867125034332275,
      "learning_rate": 3.572264319144195e-05,
      "loss": 0.6825,
      "step": 20500
    },
    {
      "epoch": 2.3400936037441498,
      "grad_norm": 5.109810829162598,
      "learning_rate": 3.537441497659906e-05,
      "loss": 0.6725,
      "step": 21000
    },
    {
      "epoch": 2.3958101181190106,
      "grad_norm": 2.966052770614624,
      "learning_rate": 3.5026186761756186e-05,
      "loss": 0.6681,
      "step": 21500
    },
    {
      "epoch": 2.4515266324938714,
      "grad_norm": 4.8155670166015625,
      "learning_rate": 3.467795854691331e-05,
      "loss": 0.6757,
      "step": 22000
    },
    {
      "epoch": 2.5072431468687317,
      "grad_norm": 3.092560052871704,
      "learning_rate": 3.4329730332070424e-05,
      "loss": 0.681,
      "step": 22500
    },
    {
      "epoch": 2.5629596612435925,
      "grad_norm": 3.422775983810425,
      "learning_rate": 3.398150211722755e-05,
      "loss": 0.6743,
      "step": 23000
    },
    {
      "epoch": 2.6186761756184533,
      "grad_norm": 5.748045444488525,
      "learning_rate": 3.363327390238467e-05,
      "loss": 0.6778,
      "step": 23500
    },
    {
      "epoch": 2.674392689993314,
      "grad_norm": 5.706899166107178,
      "learning_rate": 3.328504568754179e-05,
      "loss": 0.6759,
      "step": 24000
    },
    {
      "epoch": 2.7301092043681745,
      "grad_norm": 7.628303527832031,
      "learning_rate": 3.293681747269891e-05,
      "loss": 0.677,
      "step": 24500
    },
    {
      "epoch": 2.7858257187430353,
      "grad_norm": 11.281683921813965,
      "learning_rate": 3.258858925785603e-05,
      "loss": 0.6816,
      "step": 25000
    },
    {
      "epoch": 2.841542233117896,
      "grad_norm": 4.438465595245361,
      "learning_rate": 3.224036104301315e-05,
      "loss": 0.6728,
      "step": 25500
    },
    {
      "epoch": 2.897258747492757,
      "grad_norm": 6.848166465759277,
      "learning_rate": 3.1892132828170276e-05,
      "loss": 0.6827,
      "step": 26000
    },
    {
      "epoch": 2.9529752618676177,
      "grad_norm": 2.539121389389038,
      "learning_rate": 3.154390461332739e-05,
      "loss": 0.6748,
      "step": 26500
    },
    {
      "epoch": 3.0,
      "eval_loss": 0.6684982776641846,
      "eval_runtime": 85.1953,
      "eval_samples_per_second": 105.334,
      "eval_steps_per_second": 13.17,
      "step": 26922
    },
    {
      "epoch": 3.008691776242478,
      "grad_norm": 12.604024887084961,
      "learning_rate": 3.119567639848451e-05,
      "loss": 0.6809,
      "step": 27000
    },
    {
      "epoch": 3.064408290617339,
      "grad_norm": 6.005545616149902,
      "learning_rate": 3.084744818364163e-05,
      "loss": 0.6809,
      "step": 27500
    },
    {
      "epoch": 3.1201248049921997,
      "grad_norm": 4.102715969085693,
      "learning_rate": 3.0499219968798754e-05,
      "loss": 0.6755,
      "step": 28000
    },
    {
      "epoch": 3.1758413193670605,
      "grad_norm": 5.7648162841796875,
      "learning_rate": 3.0150991753955877e-05,
      "loss": 0.6763,
      "step": 28500
    },
    {
      "epoch": 3.2315578337419213,
      "grad_norm": 2.4880828857421875,
      "learning_rate": 2.9802763539112992e-05,
      "loss": 0.6749,
      "step": 29000
    },
    {
      "epoch": 3.2872743481167817,
      "grad_norm": 7.0493340492248535,
      "learning_rate": 2.9454535324270118e-05,
      "loss": 0.6766,
      "step": 29500
    },
    {
      "epoch": 3.3429908624916425,
      "grad_norm": 12.130148887634277,
      "learning_rate": 2.9106307109427233e-05,
      "loss": 0.681,
      "step": 30000
    },
    {
      "epoch": 3.3987073768665033,
      "grad_norm": 5.485567092895508,
      "learning_rate": 2.8758078894584355e-05,
      "loss": 0.6752,
      "step": 30500
    },
    {
      "epoch": 3.454423891241364,
      "grad_norm": 6.800590515136719,
      "learning_rate": 2.8409850679741477e-05,
      "loss": 0.6772,
      "step": 31000
    },
    {
      "epoch": 3.510140405616225,
      "grad_norm": 4.885553359985352,
      "learning_rate": 2.8061622464898596e-05,
      "loss": 0.6742,
      "step": 31500
    },
    {
      "epoch": 3.5658569199910852,
      "grad_norm": 12.828522682189941,
      "learning_rate": 2.771339425005572e-05,
      "loss": 0.6803,
      "step": 32000
    },
    {
      "epoch": 3.621573434365946,
      "grad_norm": 11.761845588684082,
      "learning_rate": 2.7365166035212837e-05,
      "loss": 0.6843,
      "step": 32500
    },
    {
      "epoch": 3.677289948740807,
      "grad_norm": 4.997180938720703,
      "learning_rate": 2.701693782036996e-05,
      "loss": 0.666,
      "step": 33000
    },
    {
      "epoch": 3.7330064631156676,
      "grad_norm": 12.41753101348877,
      "learning_rate": 2.666870960552708e-05,
      "loss": 0.6731,
      "step": 33500
    },
    {
      "epoch": 3.7887229774905284,
      "grad_norm": 13.171890258789062,
      "learning_rate": 2.6320481390684197e-05,
      "loss": 0.6814,
      "step": 34000
    },
    {
      "epoch": 3.844439491865389,
      "grad_norm": 13.550444602966309,
      "learning_rate": 2.5972253175841323e-05,
      "loss": 0.6879,
      "step": 34500
    },
    {
      "epoch": 3.9001560062402496,
      "grad_norm": 14.168880462646484,
      "learning_rate": 2.5624024960998438e-05,
      "loss": 0.6818,
      "step": 35000
    },
    {
      "epoch": 3.9558725206151104,
      "grad_norm": 2.903534412384033,
      "learning_rate": 2.527579674615556e-05,
      "loss": 0.6777,
      "step": 35500
    },
    {
      "epoch": 4.0,
      "eval_loss": 0.676997721195221,
      "eval_runtime": 84.4395,
      "eval_samples_per_second": 106.277,
      "eval_steps_per_second": 13.288,
      "step": 35896
    },
    {
      "epoch": 4.011589034989971,
      "grad_norm": 4.052273750305176,
      "learning_rate": 2.4927568531312683e-05,
      "loss": 0.6692,
      "step": 36000
    },
    {
      "epoch": 4.067305549364832,
      "grad_norm": 5.67786979675293,
      "learning_rate": 2.4579340316469805e-05,
      "loss": 0.6682,
      "step": 36500
    },
    {
      "epoch": 4.123022063739692,
      "grad_norm": 14.715605735778809,
      "learning_rate": 2.4231112101626924e-05,
      "loss": 0.6747,
      "step": 37000
    },
    {
      "epoch": 4.178738578114553,
      "grad_norm": 15.509727478027344,
      "learning_rate": 2.3882883886784042e-05,
      "loss": 0.6657,
      "step": 37500
    },
    {
      "epoch": 4.234455092489414,
      "grad_norm": 5.679556846618652,
      "learning_rate": 2.3534655671941165e-05,
      "loss": 0.6743,
      "step": 38000
    },
    {
      "epoch": 4.290171606864274,
      "grad_norm": 7.3081183433532715,
      "learning_rate": 2.3186427457098283e-05,
      "loss": 0.6832,
      "step": 38500
    },
    {
      "epoch": 4.345888121239136,
      "grad_norm": 12.229215621948242,
      "learning_rate": 2.2838199242255406e-05,
      "loss": 0.6731,
      "step": 39000
    },
    {
      "epoch": 4.401604635613996,
      "grad_norm": 4.034285545349121,
      "learning_rate": 2.2489971027412528e-05,
      "loss": 0.6774,
      "step": 39500
    },
    {
      "epoch": 4.457321149988856,
      "grad_norm": 13.6174898147583,
      "learning_rate": 2.2141742812569647e-05,
      "loss": 0.6877,
      "step": 40000
    },
    {
      "epoch": 4.513037664363718,
      "grad_norm": 15.57822322845459,
      "learning_rate": 2.1793514597726765e-05,
      "loss": 0.6793,
      "step": 40500
    },
    {
      "epoch": 4.568754178738578,
      "grad_norm": 4.168578147888184,
      "learning_rate": 2.1445286382883888e-05,
      "loss": 0.672,
      "step": 41000
    },
    {
      "epoch": 4.624470693113439,
      "grad_norm": 3.911161422729492,
      "learning_rate": 2.109705816804101e-05,
      "loss": 0.6849,
      "step": 41500
    },
    {
      "epoch": 4.6801872074882995,
      "grad_norm": 3.590224027633667,
      "learning_rate": 2.074882995319813e-05,
      "loss": 0.6693,
      "step": 42000
    },
    {
      "epoch": 4.73590372186316,
      "grad_norm": 5.390357494354248,
      "learning_rate": 2.0400601738355247e-05,
      "loss": 0.6801,
      "step": 42500
    },
    {
      "epoch": 4.791620236238021,
      "grad_norm": 5.1938018798828125,
      "learning_rate": 2.005237352351237e-05,
      "loss": 0.6741,
      "step": 43000
    },
    {
      "epoch": 4.8473367506128815,
      "grad_norm": 2.982243776321411,
      "learning_rate": 1.9704145308669492e-05,
      "loss": 0.6715,
      "step": 43500
    },
    {
      "epoch": 4.903053264987743,
      "grad_norm": 5.5764970779418945,
      "learning_rate": 1.935591709382661e-05,
      "loss": 0.6726,
      "step": 44000
    },
    {
      "epoch": 4.958769779362603,
      "grad_norm": 5.272607326507568,
      "learning_rate": 1.9007688878983733e-05,
      "loss": 0.6757,
      "step": 44500
    },
    {
      "epoch": 5.0,
      "eval_loss": 0.6684049367904663,
      "eval_runtime": 83.5979,
      "eval_samples_per_second": 107.347,
      "eval_steps_per_second": 13.421,
      "step": 44870
    },
    {
      "epoch": 5.0144862937374635,
      "grad_norm": 6.212952136993408,
      "learning_rate": 1.865946066414085e-05,
      "loss": 0.6775,
      "step": 45000
    },
    {
      "epoch": 5.070202808112325,
      "grad_norm": 5.5741143226623535,
      "learning_rate": 1.831123244929797e-05,
      "loss": 0.683,
      "step": 45500
    },
    {
      "epoch": 5.125919322487185,
      "grad_norm": 12.695657730102539,
      "learning_rate": 1.7963004234455093e-05,
      "loss": 0.6779,
      "step": 46000
    },
    {
      "epoch": 5.181635836862046,
      "grad_norm": 13.414358139038086,
      "learning_rate": 1.7614776019612215e-05,
      "loss": 0.6807,
      "step": 46500
    },
    {
      "epoch": 5.237352351236907,
      "grad_norm": 6.954864978790283,
      "learning_rate": 1.7266547804769334e-05,
      "loss": 0.6714,
      "step": 47000
    },
    {
      "epoch": 5.293068865611767,
      "grad_norm": 4.69439172744751,
      "learning_rate": 1.6918319589926453e-05,
      "loss": 0.6822,
      "step": 47500
    },
    {
      "epoch": 5.348785379986628,
      "grad_norm": 12.214035034179688,
      "learning_rate": 1.6570091375083578e-05,
      "loss": 0.6686,
      "step": 48000
    },
    {
      "epoch": 5.404501894361489,
      "grad_norm": 5.972795486450195,
      "learning_rate": 1.6221863160240697e-05,
      "loss": 0.6768,
      "step": 48500
    },
    {
      "epoch": 5.460218408736349,
      "grad_norm": 4.673641681671143,
      "learning_rate": 1.5873634945397816e-05,
      "loss": 0.6818,
      "step": 49000
    },
    {
      "epoch": 5.51593492311121,
      "grad_norm": 5.844644546508789,
      "learning_rate": 1.5525406730554938e-05,
      "loss": 0.6716,
      "step": 49500
    },
    {
      "epoch": 5.571651437486071,
      "grad_norm": 14.574623107910156,
      "learning_rate": 1.5177178515712057e-05,
      "loss": 0.6686,
      "step": 50000
    },
    {
      "epoch": 5.627367951860932,
      "grad_norm": 3.484602928161621,
      "learning_rate": 1.4828950300869179e-05,
      "loss": 0.6799,
      "step": 50500
    },
    {
      "epoch": 5.683084466235792,
      "grad_norm": 4.869457244873047,
      "learning_rate": 1.44807220860263e-05,
      "loss": 0.6751,
      "step": 51000
    },
    {
      "epoch": 5.7388009806106535,
      "grad_norm": 6.234749794006348,
      "learning_rate": 1.413249387118342e-05,
      "loss": 0.686,
      "step": 51500
    },
    {
      "epoch": 5.794517494985514,
      "grad_norm": 5.295376777648926,
      "learning_rate": 1.3784265656340539e-05,
      "loss": 0.6745,
      "step": 52000
    },
    {
      "epoch": 5.850234009360374,
      "grad_norm": 3.5709116458892822,
      "learning_rate": 1.343603744149766e-05,
      "loss": 0.665,
      "step": 52500
    },
    {
      "epoch": 5.905950523735235,
      "grad_norm": 5.617607116699219,
      "learning_rate": 1.3087809226654782e-05,
      "loss": 0.6641,
      "step": 53000
    },
    {
      "epoch": 5.961667038110096,
      "grad_norm": 6.895483493804932,
      "learning_rate": 1.2739581011811902e-05,
      "loss": 0.6745,
      "step": 53500
    },
    {
      "epoch": 6.0,
      "eval_loss": 0.6688558459281921,
      "eval_runtime": 83.3024,
      "eval_samples_per_second": 107.728,
      "eval_steps_per_second": 13.469,
      "step": 53844
    },
    {
      "epoch": 6.017383552484956,
      "grad_norm": 12.799263000488281,
      "learning_rate": 1.2391352796969023e-05,
      "loss": 0.6768,
      "step": 54000
    },
    {
      "epoch": 6.073100066859817,
      "grad_norm": 3.696225881576538,
      "learning_rate": 1.2043124582126143e-05,
      "loss": 0.6767,
      "step": 54500
    },
    {
      "epoch": 6.128816581234678,
      "grad_norm": 13.979578971862793,
      "learning_rate": 1.1694896367283264e-05,
      "loss": 0.6725,
      "step": 55000
    },
    {
      "epoch": 6.184533095609539,
      "grad_norm": 4.764008045196533,
      "learning_rate": 1.1346668152440384e-05,
      "loss": 0.6732,
      "step": 55500
    },
    {
      "epoch": 6.240249609984399,
      "grad_norm": 10.908492088317871,
      "learning_rate": 1.0998439937597505e-05,
      "loss": 0.6759,
      "step": 56000
    },
    {
      "epoch": 6.29596612435926,
      "grad_norm": 6.591485500335693,
      "learning_rate": 1.0650211722754625e-05,
      "loss": 0.6698,
      "step": 56500
    },
    {
      "epoch": 6.351682638734121,
      "grad_norm": 5.322215557098389,
      "learning_rate": 1.0301983507911746e-05,
      "loss": 0.6751,
      "step": 57000
    },
    {
      "epoch": 6.407399153108981,
      "grad_norm": 4.779172897338867,
      "learning_rate": 9.953755293068866e-06,
      "loss": 0.6766,
      "step": 57500
    },
    {
      "epoch": 6.463115667483843,
      "grad_norm": 6.307823181152344,
      "learning_rate": 9.605527078225987e-06,
      "loss": 0.6675,
      "step": 58000
    },
    {
      "epoch": 6.518832181858703,
      "grad_norm": 4.298532009124756,
      "learning_rate": 9.257298863383107e-06,
      "loss": 0.676,
      "step": 58500
    },
    {
      "epoch": 6.574548696233563,
      "grad_norm": 5.397825241088867,
      "learning_rate": 8.909070648540228e-06,
      "loss": 0.6762,
      "step": 59000
    },
    {
      "epoch": 6.6302652106084246,
      "grad_norm": 6.405599594116211,
      "learning_rate": 8.560842433697348e-06,
      "loss": 0.6739,
      "step": 59500
    },
    {
      "epoch": 6.685981724983285,
      "grad_norm": 6.03638219833374,
      "learning_rate": 8.212614218854469e-06,
      "loss": 0.6778,
      "step": 60000
    },
    {
      "epoch": 6.741698239358145,
      "grad_norm": 5.288705348968506,
      "learning_rate": 7.86438600401159e-06,
      "loss": 0.6807,
      "step": 60500
    },
    {
      "epoch": 6.7974147537330065,
      "grad_norm": 7.005372524261475,
      "learning_rate": 7.51615778916871e-06,
      "loss": 0.6729,
      "step": 61000
    },
    {
      "epoch": 6.853131268107867,
      "grad_norm": 12.860966682434082,
      "learning_rate": 7.167929574325831e-06,
      "loss": 0.6782,
      "step": 61500
    },
    {
      "epoch": 6.908847782482728,
      "grad_norm": 3.9950733184814453,
      "learning_rate": 6.819701359482951e-06,
      "loss": 0.6689,
      "step": 62000
    },
    {
      "epoch": 6.9645642968575885,
      "grad_norm": 4.556290626525879,
      "learning_rate": 6.471473144640072e-06,
      "loss": 0.6736,
      "step": 62500
    },
    {
      "epoch": 7.0,
      "eval_loss": 0.6713135838508606,
      "eval_runtime": 82.748,
      "eval_samples_per_second": 108.45,
      "eval_steps_per_second": 13.559,
      "step": 62818
    },
    {
      "epoch": 7.02028081123245,
      "grad_norm": 5.325989246368408,
      "learning_rate": 6.123244929797193e-06,
      "loss": 0.6763,
      "step": 63000
    },
    {
      "epoch": 7.07599732560731,
      "grad_norm": 5.4230780601501465,
      "learning_rate": 5.775016714954313e-06,
      "loss": 0.6726,
      "step": 63500
    },
    {
      "epoch": 7.1317138399821705,
      "grad_norm": 4.110693454742432,
      "learning_rate": 5.426788500111434e-06,
      "loss": 0.6735,
      "step": 64000
    },
    {
      "epoch": 7.187430354357032,
      "grad_norm": 3.7261908054351807,
      "learning_rate": 5.078560285268554e-06,
      "loss": 0.6793,
      "step": 64500
    },
    {
      "epoch": 7.243146868731892,
      "grad_norm": 4.381598472595215,
      "learning_rate": 4.730332070425674e-06,
      "loss": 0.6756,
      "step": 65000
    },
    {
      "epoch": 7.298863383106752,
      "grad_norm": 4.5578436851501465,
      "learning_rate": 4.382103855582795e-06,
      "loss": 0.6667,
      "step": 65500
    },
    {
      "epoch": 7.354579897481614,
      "grad_norm": 7.42918586730957,
      "learning_rate": 4.033875640739916e-06,
      "loss": 0.6697,
      "step": 66000
    },
    {
      "epoch": 7.410296411856474,
      "grad_norm": 11.539424896240234,
      "learning_rate": 3.685647425897036e-06,
      "loss": 0.6717,
      "step": 66500
    },
    {
      "epoch": 7.466012926231335,
      "grad_norm": 4.009253025054932,
      "learning_rate": 3.3374192110541567e-06,
      "loss": 0.6733,
      "step": 67000
    },
    {
      "epoch": 7.521729440606196,
      "grad_norm": 12.21474552154541,
      "learning_rate": 2.989190996211277e-06,
      "loss": 0.6715,
      "step": 67500
    },
    {
      "epoch": 7.577445954981056,
      "grad_norm": 6.2433247566223145,
      "learning_rate": 2.6409627813683977e-06,
      "loss": 0.6686,
      "step": 68000
    },
    {
      "epoch": 7.633162469355917,
      "grad_norm": 6.338887691497803,
      "learning_rate": 2.292734566525518e-06,
      "loss": 0.6837,
      "step": 68500
    },
    {
      "epoch": 7.688878983730778,
      "grad_norm": 11.169124603271484,
      "learning_rate": 1.9445063516826387e-06,
      "loss": 0.6724,
      "step": 69000
    },
    {
      "epoch": 7.744595498105639,
      "grad_norm": 12.718642234802246,
      "learning_rate": 1.5962781368397592e-06,
      "loss": 0.6753,
      "step": 69500
    },
    {
      "epoch": 7.800312012480499,
      "grad_norm": 4.798284530639648,
      "learning_rate": 1.24804992199688e-06,
      "loss": 0.6784,
      "step": 70000
    },
    {
      "epoch": 7.85602852685536,
      "grad_norm": 4.341782569885254,
      "learning_rate": 8.998217071540005e-07,
      "loss": 0.673,
      "step": 70500
    },
    {
      "epoch": 7.911745041230221,
      "grad_norm": 5.809617042541504,
      "learning_rate": 5.515934923111211e-07,
      "loss": 0.6722,
      "step": 71000
    },
    {
      "epoch": 7.967461555605081,
      "grad_norm": 4.555985927581787,
      "learning_rate": 2.0336527746824158e-07,
      "loss": 0.6697,
      "step": 71500
    }
  ],
  "logging_steps": 500,
  "max_steps": 71792,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 8,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 7.555286576873472e+16,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
