{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad8ca83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3abe58c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/kalyankumar/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/kalyankumar/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/kalyankumar/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Downloading necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51e7d6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1: Loading the datasets\n",
    "#Ensuring that the dataset has a label column indicating 'machine-generated' or 'user-generated'\n",
    "train_df = pd.read_csv('/Users/kalyankumar/Downloads/archive (2)/CSV/train.csv')\n",
    "val_df = pd.read_csv('/Users/kalyankumar/Downloads/archive (2)/CSV/validation.csv')\n",
    "test_df = pd.read_csv('/Users/kalyankumar/Downloads/archive (2)/CSV/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56ea627d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in 'dialogue' column before cleaning: 0\n",
      "Missing values in 'dialogue' column after cleaning: 0\n",
      "Missing values in 'dialogue' column before cleaning: 0\n",
      "Missing values in 'dialogue' column after cleaning: 0\n",
      "Missing values in 'dialogue' column before cleaning: 0\n",
      "Missing values in 'dialogue' column after cleaning: 0\n"
     ]
    }
   ],
   "source": [
    "#Step 2: Handling Missing Data\n",
    "def handle_missing_data(df, text_column):\n",
    "    print(f\"Missing values in '{text_column}' column before cleaning: {df[text_column].isnull().sum()}\")\n",
    "    df = df.dropna(subset=[text_column])  # Drop rows with missing text data\n",
    "    print(f\"Missing values in '{text_column}' column after cleaning: {df[text_column].isnull().sum()}\")\n",
    "    return df\n",
    "\n",
    "# Applying missing data handling to each dataset\n",
    "train_df = handle_missing_data(train_df, 'dialogue')\n",
    "val_df = handle_missing_data(val_df, 'dialogue')\n",
    "test_df = handle_missing_data(test_df, 'dialogue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9f772f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 3: Text Cleaning\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text)  # Removing non-alphabetical characters\n",
    "    text = text.strip()  # Removing leading and trailing whitespace\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eec4bb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 4: Preprocessing Text\n",
    "def preprocess_text(text):\n",
    "\n",
    "#Text cleaning\n",
    "    text = clean_text(text)\n",
    "    \n",
    "#Tokenization and Lowercasing\n",
    "    tokens = word_tokenize(text.lower())  # Tokenizing and convert to lowercase\n",
    "    \n",
    "#Removing Stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "#Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    return ' '.join(tokens)  # Joining tokens back into a single string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e13d484b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in 'dialogue' column before cleaning: 0\n",
      "Missing values in 'dialogue' column after cleaning: 0\n",
      "Missing values in 'dialogue' column before cleaning: 0\n",
      "Missing values in 'dialogue' column after cleaning: 0\n",
      "Missing values in 'dialogue' column before cleaning: 0\n",
      "Missing values in 'dialogue' column after cleaning: 0\n"
     ]
    }
   ],
   "source": [
    "#Step 5: Full Preprocessing Workflow for each dataset\n",
    "def preprocess_dataset(df, text_column):\n",
    "#Handling missing data\n",
    "    df = handle_missing_data(df, text_column)\n",
    "    \n",
    "#Applying text preprocessing\n",
    "    df['processed_text'] = df[text_column].apply(preprocess_text)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply preprocessing to each dataset\n",
    "train_df = preprocess_dataset(train_df, 'dialogue')\n",
    "val_df = preprocess_dataset(val_df, 'dialogue')\n",
    "test_df = preprocess_dataset(test_df, 'dialogue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d188663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data:\n",
      "                                            dialogue  \\\n",
      "0  #Person1#: Hi, Mr. Smith. I'm Doctor Hawkins. ...   \n",
      "1  #Person1#: Hello Mrs. Parker, how have you bee...   \n",
      "2  #Person1#: Excuse me, did you see a set of key...   \n",
      "3  #Person1#: Why didn't you tell me you had a gi...   \n",
      "4  #Person1#: Watsup, ladies! Y'll looking'fine t...   \n",
      "\n",
      "                                      processed_text  \n",
      "0  person hi mr smith im doctor hawkins today per...  \n",
      "1  person hello mr parker person hello dr peter f...  \n",
      "2  person excuse see set key person kind key pers...  \n",
      "3  person didnt tell girlfriend person sorry thou...  \n",
      "4  person watsup lady yll lookingfine tonight may...  \n"
     ]
    }
   ],
   "source": [
    "print(\"Training Data:\")\n",
    "print(train_df[['dialogue', 'processed_text']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c9e6dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Data:\n",
      "                                            dialogue  \\\n",
      "0  #Person1#: Hello, how are you doing today?\\n#P...   \n",
      "1  #Person1#: Hey Jimmy. Let's go workout later t...   \n",
      "2  #Person1#: I need to stop eating such unhealth...   \n",
      "3  #Person1#: Do you believe in UFOs?\\n#Person2#:...   \n",
      "4  #Person1#: Did you go to school today?\\n#Perso...   \n",
      "\n",
      "                                      processed_text  \n",
      "0  person hello today person trouble breathing la...  \n",
      "1  person hey jimmy let go workout later today pe...  \n",
      "2  person need stop eating unhealthy food person ...  \n",
      "3  person believe ufo person course person never ...  \n",
      "4  person go school today person course person di...  \n"
     ]
    }
   ],
   "source": [
    "print(\"\\nValidation Data:\")\n",
    "print(val_df[['dialogue', 'processed_text']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "313880f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Data:\n",
      "                                            dialogue  \\\n",
      "0  #Person1#: Ms. Dawson, I need you to take a di...   \n",
      "1  #Person1#: Ms. Dawson, I need you to take a di...   \n",
      "2  #Person1#: Ms. Dawson, I need you to take a di...   \n",
      "3  #Person1#: You're finally here! What took so l...   \n",
      "4  #Person1#: You're finally here! What took so l...   \n",
      "\n",
      "                                      processed_text  \n",
      "0  person m dawson need take dictation person yes...  \n",
      "1  person m dawson need take dictation person yes...  \n",
      "2  person m dawson need take dictation person yes...  \n",
      "3  person youre finally took long person got stuc...  \n",
      "4  person youre finally took long person got stuc...  \n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTest Data:\")\n",
    "print(test_df[['dialogue', 'processed_text']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36a85ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exporting the datasets individually\n",
    "\n",
    "train_df.to_csv('/Users/kalyankumar/Downloads/train_processed.csv', index=False)\n",
    "val_df.to_csv('/Users/kalyankumar/Downloads/val_processed.csv', index=False)\n",
    "test_df.to_csv('/Users/kalyankumar/Downloads/test_processed.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
